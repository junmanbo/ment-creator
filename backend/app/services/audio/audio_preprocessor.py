"""
ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì„œë¹„ìŠ¤

Voice Cloningì„ ìœ„í•œ ìµœì ì˜ ìŒì„± ìƒ˜í”Œì„ ë§Œë“¤ê¸° ìœ„í•œ ì „ì²˜ë¦¬ ê¸°ëŠ¥ë“¤
"""
import numpy as np
import librosa
import soundfile as sf
import noisereduce as nr
from scipy.signal import butter, lfilter, savgol_filter
from scipy.ndimage import median_filter
import pyloudnorm as pyln
from typing import Optional, Tuple, Dict, Any, List
import logging
from pathlib import Path
import json

logger = logging.getLogger(__name__)


class AudioPreprocessor:
    """Voice Cloningì„ ìœ„í•œ ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, target_sr: int = 22050, target_lufs: float = -23.0):
        """
        Args:
            target_sr: ëª©í‘œ ìƒ˜í”Œë§ ë ˆì´íŠ¸ (ê¸°ë³¸ê°’: 22050Hz - Coqui TTS ê¶Œì¥)
            target_lufs: ëª©í‘œ LUFS (Loudness Units relative to Full Scale) ê°’
        """
        self.target_sr = target_sr
        self.target_lufs = target_lufs
        self.meter = pyln.Meter(target_sr)  # ë¼ìš°ë“œë‹ˆìŠ¤ ë¯¸í„°
        
    def process_audio(
        self,
        input_path: str,
        output_path: str,
        config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ìŒì„± íŒŒì¼ì„ Voice Cloningì— ìµœì í™”ëœ í˜•íƒœë¡œ ì „ì²˜ë¦¬
        
        Args:
            input_path: ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            output_path: ì¶œë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            config: ì „ì²˜ë¦¬ ì„¤ì • (ì„ íƒì‚¬í•­)
            
        Returns:
            ì „ì²˜ë¦¬ ê²°ê³¼ ì •ë³´
        """
        # ê¸°ë³¸ ì„¤ì •
        default_config = {
            "denoise": True,
            "denoise_strength": 0.7,
            "trim_silence": True,
            "silence_threshold": -40,  # dB
            "normalize": True,
            "remove_low_freq": True,
            "low_freq_cutoff": 80,  # Hz
            "remove_high_freq": True,
            "high_freq_cutoff": 8000,  # Hz
            "apply_compression": True,
            "compression_ratio": 0.8,
            "enhance_clarity": True
        }
        
        if config:
            default_config.update(config)
        config = default_config
        
        try:
            # 1. ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
            audio, sr = librosa.load(input_path, sr=None, mono=True)
            logger.info(f"ë¡œë“œëœ ì˜¤ë””ì˜¤: {len(audio)} ìƒ˜í”Œ, {sr}Hz")
            
            # 2. ë¦¬ìƒ˜í”Œë§ (í•„ìš”í•œ ê²½ìš°)
            if sr != self.target_sr:
                audio = librosa.resample(audio, orig_sr=sr, target_sr=self.target_sr)
                sr = self.target_sr
                logger.info(f"ë¦¬ìƒ˜í”Œë§ ì™„ë£Œ: {self.target_sr}Hz")
            
            # 3. ë…¸ì´ì¦ˆ ì œê±°
            if config["denoise"]:
                audio = self._denoise_audio(audio, sr, config["denoise_strength"])
                logger.info("ë…¸ì´ì¦ˆ ì œê±° ì™„ë£Œ")
            
            # 4. ì¹¨ë¬µ êµ¬ê°„ ì œê±°
            if config["trim_silence"]:
                audio = self._trim_silence(audio, sr, config["silence_threshold"])
                logger.info("ì¹¨ë¬µ êµ¬ê°„ ì œê±° ì™„ë£Œ")
            
            # 5. ì£¼íŒŒìˆ˜ í•„í„°ë§
            if config["remove_low_freq"] or config["remove_high_freq"]:
                audio = self._apply_frequency_filter(
                    audio, sr,
                    config["low_freq_cutoff"] if config["remove_low_freq"] else None,
                    config["high_freq_cutoff"] if config["remove_high_freq"] else None
                )
                logger.info("ì£¼íŒŒìˆ˜ í•„í„°ë§ ì™„ë£Œ")
            
            # 6. ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€ ì••ì¶•
            if config["apply_compression"]:
                audio = self._apply_compression(audio, config["compression_ratio"])
                logger.info("ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€ ì••ì¶• ì™„ë£Œ")
            
            # 7. ìŒì„± ëª…ë£Œë„ í–¥ìƒ
            if config["enhance_clarity"]:
                audio = self._enhance_clarity(audio, sr)
                logger.info("ìŒì„± ëª…ë£Œë„ í–¥ìƒ ì™„ë£Œ")
            
            # 8. ë¼ìš°ë“œë‹ˆìŠ¤ ì •ê·œí™” (LUFS ê¸°ì¤€)
            if config["normalize"]:
                audio = self._normalize_loudness(audio, sr)
                logger.info(f"ë¼ìš°ë“œë‹ˆìŠ¤ ì •ê·œí™” ì™„ë£Œ: {self.target_lufs} LUFS")
            
            # 9. í´ë¦¬í•‘ ë°©ì§€
            audio = self._prevent_clipping(audio)
            
            # 10. ì €ì¥
            sf.write(output_path, audio, sr, subtype='PCM_16')
            logger.info(f"ì „ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ ì €ì¥ ì™„ë£Œ: {output_path}")
            
            # ê²°ê³¼ ë¶„ì„
            result = self._analyze_audio(audio, sr)
            result["config"] = config
            result["input_file"] = input_path
            result["output_file"] = output_path
            
            return result
            
        except Exception as e:
            logger.error(f"ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise
    
    def _denoise_audio(
        self,
        audio: np.ndarray,
        sr: int,
        strength: float = 0.7
    ) -> np.ndarray:
        """ê³ ê¸‰ ë…¸ì´ì¦ˆ ì œê±°"""
        # stationary noise reduction
        denoised = nr.reduce_noise(
            y=audio,
            sr=sr,
            stationary=True,
            prop_decrease=strength
        )
        
        # ì¶”ê°€ì ì¸ median í•„í„°ë¡œ í´ë¦­ ë…¸ì´ì¦ˆ ì œê±°
        denoised = median_filter(denoised, size=3)
        
        return denoised
    
    def _trim_silence(
        self,
        audio: np.ndarray,
        sr: int,
        threshold_db: float = -40
    ) -> np.ndarray:
        """ì¹¨ë¬µ êµ¬ê°„ ì œê±° (ì‹œì‘ê³¼ ë)"""
        # ì¹¨ë¬µ êµ¬ê°„ ì°¾ê¸°
        intervals = librosa.effects.split(
            audio,
            top_db=-threshold_db,
            frame_length=2048,
            hop_length=512
        )
        
        if len(intervals) > 0:
            # ìœ íš¨í•œ êµ¬ê°„ë“¤ì„ ì—°ê²°
            trimmed_audio = []
            for start, end in intervals:
                trimmed_audio.append(audio[start:end])
            
            # êµ¬ê°„ ì‚¬ì´ì— ì§§ì€ ì¹¨ë¬µ ì¶”ê°€ (ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°)
            silence_duration = int(0.1 * sr)  # 0.1ì´ˆ
            silence = np.zeros(silence_duration)
            
            result = []
            for i, segment in enumerate(trimmed_audio):
                result.append(segment)
                if i < len(trimmed_audio) - 1:
                    result.append(silence)
            
            return np.concatenate(result)
        
        return audio
    
    def _apply_frequency_filter(
        self,
        audio: np.ndarray,
        sr: int,
        low_cutoff: Optional[float] = None,
        high_cutoff: Optional[float] = None
    ) -> np.ndarray:
        """ì£¼íŒŒìˆ˜ ëŒ€ì—­ í•„í„°ë§"""
        nyquist = sr / 2
        
        if low_cutoff and high_cutoff:
            # ë°´ë“œíŒ¨ìŠ¤ í•„í„°
            low = low_cutoff / nyquist
            high = high_cutoff / nyquist
            b, a = butter(4, [low, high], btype='band')
        elif low_cutoff:
            # í•˜ì´íŒ¨ìŠ¤ í•„í„°
            low = low_cutoff / nyquist
            b, a = butter(4, low, btype='high')
        elif high_cutoff:
            # ë¡œìš°íŒ¨ìŠ¤ í•„í„°
            high = high_cutoff / nyquist
            b, a = butter(4, high, btype='low')
        else:
            return audio
        
        # í•„í„° ì ìš©
        filtered = lfilter(b, a, audio)
        
        # ìœ„ìƒ ì™œê³¡ ë³´ì •ì„ ìœ„í•´ ì—­ë°©í–¥ í•„í„°ë§ë„ ì ìš©
        filtered = lfilter(b, a, filtered[::-1])[::-1]
        
        return filtered
    
    def _apply_compression(
        self,
        audio: np.ndarray,
        ratio: float = 0.8
    ) -> np.ndarray:
        """ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€ ì••ì¶•"""
        # RMS ê³„ì‚°
        frame_length = 2048
        hop_length = 512
        rms = librosa.feature.rms(
            y=audio,
            frame_length=frame_length,
            hop_length=hop_length
        )[0]
        
        # ìŠ¤ë¬´ë”©
        rms_smooth = savgol_filter(rms, 31, 3)
        
        # ì••ì¶• ì»¤ë¸Œ ì ìš©
        threshold = np.percentile(rms_smooth, 70)
        gain = np.ones_like(rms_smooth)
        
        above_threshold = rms_smooth > threshold
        gain[above_threshold] = (
            threshold / rms_smooth[above_threshold]
        ) ** (1 - ratio)
        
        # ê²Œì¸ ì ìš©
        gain_interp = np.interp(
            np.arange(len(audio)),
            np.arange(len(gain)) * hop_length,
            gain
        )
        
        compressed = audio * gain_interp
        
        return compressed
    
    def _enhance_clarity(
        self,
        audio: np.ndarray,
        sr: int
    ) -> np.ndarray:
        """ìŒì„± ëª…ë£Œë„ í–¥ìƒ"""
        # 1. ìŠ¤í™íŠ¸ëŸ´ ì„¼íŠ¸ë¡œì´ë“œ ê¸°ë°˜ í–¥ìƒ
        spectral_centroids = librosa.feature.spectral_centroid(
            y=audio, sr=sr
        )[0]
        
        # 2. í¬ë¨¼íŠ¸ ê°•ì¡° (2-4kHz ëŒ€ì—­)
        formant_enhanced = self._apply_frequency_filter(
            audio, sr, 2000, 4000
        )
        
        # 3. ì›ë³¸ê³¼ ë¸”ë Œë”©
        blend_ratio = 0.3
        enhanced = audio * (1 - blend_ratio) + formant_enhanced * blend_ratio
        
        # 4. ë¯¸ì„¸í•œ í•˜ëª¨ë‹‰ í–¥ìƒ
        harmonic, percussive = librosa.effects.hpss(enhanced)
        enhanced = harmonic * 1.1 + percussive * 0.9
        
        return enhanced
    
    def _normalize_loudness(
        self,
        audio: np.ndarray,
        sr: int
    ) -> np.ndarray:
        """LUFS ê¸°ì¤€ ë¼ìš°ë“œë‹ˆìŠ¤ ì •ê·œí™”"""
        # í˜„ì¬ ë¼ìš°ë“œë‹ˆìŠ¤ ì¸¡ì •
        loudness = self.meter.integrated_loudness(audio)
        
        # ì •ê·œí™” ê²Œì¸ ê³„ì‚°
        loudness_gain_db = self.target_lufs - loudness
        loudness_gain = 10.0 ** (loudness_gain_db / 20.0)
        
        # ê²Œì¸ ì ìš©
        normalized = audio * loudness_gain
        
        return normalized
    
    def _prevent_clipping(
        self,
        audio: np.ndarray,
        headroom_db: float = -0.3
    ) -> np.ndarray:
        """í´ë¦¬í•‘ ë°©ì§€"""
        # í”¼í¬ ë ˆë²¨ í™•ì¸
        peak = np.max(np.abs(audio))
        
        # í—¤ë“œë£¸ í™•ë³´
        max_level = 10 ** (headroom_db / 20.0)
        
        if peak > max_level:
            audio = audio * (max_level / peak)
        
        # ì†Œí”„íŠ¸ ë¦¬ë¯¸í„° ì ìš©
        audio = np.tanh(audio * 0.8) / 0.8
        
        return audio
    
    def _analyze_audio(
        self,
        audio: np.ndarray,
        sr: int
    ) -> Dict[str, Any]:
        """ì˜¤ë””ì˜¤ ë¶„ì„ ì •ë³´ ìƒì„±"""
        # ê¸°ë³¸ ì •ë³´
        duration = len(audio) / sr
        
        # ë¼ìš°ë“œë‹ˆìŠ¤ ë¶„ì„
        loudness = self.meter.integrated_loudness(audio)
        
        # ì£¼íŒŒìˆ˜ ë¶„ì„
        spectral_centroid = np.mean(
            librosa.feature.spectral_centroid(y=audio, sr=sr)
        )
        
        # RMS ì—ë„ˆì§€
        rms = np.sqrt(np.mean(audio ** 2))
        
        # ì‹ í˜¸ ëŒ€ ì¡ìŒë¹„ ì¶”ì •
        noise_floor = np.percentile(np.abs(audio), 10)
        signal_peak = np.percentile(np.abs(audio), 90)
        snr_estimate = 20 * np.log10(signal_peak / (noise_floor + 1e-10))
        
        return {
            "duration_seconds": round(duration, 2),
            "sample_rate": sr,
            "loudness_lufs": round(loudness, 2),
            "spectral_centroid_hz": round(spectral_centroid, 2),
            "rms_energy": round(float(rms), 4),
            "snr_estimate_db": round(snr_estimate, 2),
            "peak_level": round(float(np.max(np.abs(audio))), 4),
            "quality_score": self._calculate_quality_score(
                loudness, snr_estimate, duration
            )
        }
    
    def _calculate_quality_score(
        self,
        loudness: float,
        snr: float,
        duration: float
    ) -> float:
        """Voice Cloning ì í•©ì„± ì ìˆ˜ ê³„ì‚° (0-100)"""
        scores = []
        
        # ë¼ìš°ë“œë‹ˆìŠ¤ ì ìˆ˜ (ëª©í‘œê°’ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        loudness_diff = abs(loudness - self.target_lufs)
        loudness_score = max(0, 100 - loudness_diff * 5)
        scores.append(loudness_score)
        
        # SNR ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        snr_score = min(100, max(0, snr * 2))
        scores.append(snr_score)
        
        # ê¸¸ì´ ì ìˆ˜ (3-30ì´ˆê°€ ì´ìƒì )
        if 3 <= duration <= 30:
            duration_score = 100
        elif duration < 3:
            duration_score = duration / 3 * 100
        else:
            duration_score = max(0, 100 - (duration - 30) * 2)
        scores.append(duration_score)
        
        # ê°€ì¤‘ í‰ê· 
        weights = [0.3, 0.5, 0.2]  # ë¼ìš°ë“œë‹ˆìŠ¤, SNR, ê¸¸ì´
        final_score = sum(s * w for s, w in zip(scores, weights))
        
        return round(final_score, 1)
    
    def batch_process(
        self,
        input_dir: str,
        output_dir: str,
        config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Dict[str, Any]]:
        """ì—¬ëŸ¬ íŒŒì¼ ì¼ê´„ ì²˜ë¦¬"""
        input_path = Path(input_dir)
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        results = {}
        
        # ì§€ì›í•˜ëŠ” ì˜¤ë””ì˜¤ í™•ì¥ì
        audio_extensions = {'.wav', '.mp3', '.flac', '.ogg', '.m4a'}
        
        for audio_file in input_path.iterdir():
            if audio_file.suffix.lower() in audio_extensions:
                try:
                    output_file = output_path / f"{audio_file.stem}_processed.wav"
                    result = self.process_audio(
                        str(audio_file),
                        str(output_file),
                        config
                    )
                    results[audio_file.name] = result
                    logger.info(f"ì²˜ë¦¬ ì™„ë£Œ: {audio_file.name}")
                except Exception as e:
                    logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {audio_file.name}: {str(e)}")
                    results[audio_file.name] = {"error": str(e)}
        
        # ê²°ê³¼ ì €ì¥
        result_file = output_path / "preprocessing_results.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        return results
    
    def preprocess_for_voice_cloning(
        self,
        input_path: str,
        apply_all: bool = True,
        output_path: Optional[str] = None
    ) -> Tuple[str, Dict[str, Any]]:
        """Voice Cloningì— ìµœì í™”ëœ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        if output_path is None:
            input_p = Path(input_path)
            output_dir = Path("preprocessed_audio")
            output_dir.mkdir(exist_ok=True)
            output_path = str(output_dir / f"{input_p.stem}_processed.wav")
        
        config = {
            "denoise": apply_all,
            "denoise_strength": 0.7,
            "trim_silence": apply_all,
            "silence_threshold": -40,
            "normalize": apply_all,
            "remove_low_freq": apply_all,
            "low_freq_cutoff": 80,
            "remove_high_freq": apply_all,
            "high_freq_cutoff": 8000,
            "apply_compression": apply_all,
            "compression_ratio": 0.8,
            "enhance_clarity": apply_all
        }
        
        result = self.process_audio(input_path, output_path, config)
        
        processing_info = {
            "applied_processes": [
                key for key, value in config.items() 
                if value is True and not key.endswith("_cutoff") and not key.endswith("_threshold")
            ],
            "quality_score": result.get("quality_score", 0),
            "duration": result.get("duration_seconds", 0),
            "sample_rate": result.get("sample_rate", self.target_sr)
        }
        
        return output_path, processing_info
    
    def analyze_audio(self, file_path: str) -> Dict[str, Any]:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            audio, sr = librosa.load(file_path, sr=None, mono=False)
            
            # ëª¨ë…¸ë¡œ ë³€í™˜ (ìŠ¤í…Œë ˆì˜¤ì¸ ê²½ìš°)
            if audio.ndim > 1:
                audio_mono = librosa.to_mono(audio)
                channels = audio.shape[0]
            else:
                audio_mono = audio
                channels = 1
            
            # ê¸°ë³¸ ì •ë³´
            duration = len(audio_mono) / sr
            
            # ì§„í­ ë¶„ì„
            peak_amplitude = np.max(np.abs(audio_mono))
            rms_level = np.sqrt(np.mean(audio_mono ** 2))
            db_level = 20 * np.log10(rms_level + 1e-10)
            
            # í´ë¦¬í•‘ ê²€ì‚¬
            clipping_samples = np.sum(np.abs(audio_mono) >= 0.99)
            
            # ì¹¨ë¬µ ë¹„ìœ¨ ê³„ì‚°
            silence_threshold = 0.01
            silence_samples = np.sum(np.abs(audio_mono) < silence_threshold)
            silence_ratio = silence_samples / len(audio_mono)
            
            # ì£¼íŒŒìˆ˜ ë¶„ì„
            spectral_centroid = np.mean(
                librosa.feature.spectral_centroid(y=audio_mono, sr=sr)
            )
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = self._calculate_quality_score_from_analysis(
                db_level, silence_ratio, duration, clipping_samples, peak_amplitude
            )
            
            return {
                "sample_rate": sr,
                "duration": duration,
                "channels": channels,
                "peak_amplitude": float(peak_amplitude),
                "rms_level": float(rms_level),
                "db_level": float(db_level),
                "clipping_samples": int(clipping_samples),
                "silence_ratio": float(silence_ratio),
                "spectral_centroid": float(spectral_centroid),
                "quality_score": quality_score
            }
            
        except Exception as e:
            logger.error(f"Audio analysis failed: {e}")
            raise
    
    def recommend_improvements(self, analysis: Dict[str, Any]) -> List[str]:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„  ì‚¬í•­ì„ ì¶”ì²œí•©ë‹ˆë‹¤."""
        recommendations = []
        
        # ë³¼ë¥¨ ë ˆë²¨ ì²´í¬
        if analysis["db_level"] < -30:
            recommendations.append("ğŸ”Š ë³¼ë¥¨ì´ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤. ì •ê·œí™”ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
        elif analysis["db_level"] > -10:
            recommendations.append("ğŸ”Š ë³¼ë¥¨ì´ ë„ˆë¬´ ë†’ìŠµë‹ˆë‹¤. ì••ì¶•ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        # í´ë¦¬í•‘ ì²´í¬
        if analysis["clipping_samples"] > 0:
            recommendations.append("âš ï¸ í´ë¦¬í•‘ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ë³¼ë¥¨ì„ ë‚®ì¶”ì„¸ìš”.")
        
        # ì¹¨ë¬µ ë¹„ìœ¨ ì²´í¬
        if analysis["silence_ratio"] > 0.3:
            recommendations.append("ğŸ¤« ì¹¨ë¬µ êµ¬ê°„ì´ ë§ìŠµë‹ˆë‹¤. íŠ¸ë¦¬ë°ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        # ê¸¸ì´ ì²´í¬
        if analysis["duration"] < 3:
            recommendations.append("â±ï¸ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. 3ì´ˆ ì´ìƒì˜ ìƒ˜í”Œì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        elif analysis["duration"] > 30:
            recommendations.append("â±ï¸ ë„ˆë¬´ ê¹ë‹ˆë‹¤. 30ì´ˆ ì´ë‚´ë¡œ í¸ì§‘í•˜ì„¸ìš”.")
        
        # ìƒ˜í”Œë ˆì´íŠ¸ ì²´í¬
        if analysis["sample_rate"] != 22050:
            recommendations.append(f"ğŸµ ìƒ˜í”Œë ˆì´íŠ¸ë¥¼ 22050Hzë¡œ ë³€ê²½ì„ ê¶Œì¥í•©ë‹ˆë‹¤. (í˜„ì¬: {analysis['sample_rate']}Hz)")
        
        # í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ì¶”ì²œ
        if analysis["quality_score"] < 70:
            recommendations.append("ğŸ¯ ì „ì²´ì ì¸ í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤. ì „ì²˜ë¦¬ë¥¼ ì ìš©í•˜ì„¸ìš”.")
        elif analysis["quality_score"] >= 90:
            recommendations.append("âœ¨ ìš°ìˆ˜í•œ í’ˆì§ˆì…ë‹ˆë‹¤! Voice Cloningì— ì í•©í•©ë‹ˆë‹¤.")
        
        return recommendations
    
    def _calculate_quality_score_from_analysis(
        self,
        db_level: float,
        silence_ratio: float,
        duration: float,
        clipping_samples: int,
        peak_amplitude: float
    ) -> float:
        """ë¶„ì„ ê²°ê³¼ë¡œë¶€í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        scores = []
        
        # ë³¼ë¥¨ ì ìˆ˜ (ëª©í‘œ: -23dB)
        volume_diff = abs(db_level - (-23))
        volume_score = max(0, 100 - volume_diff * 3)
        scores.append(volume_score)
        
        # ì¹¨ë¬µ ë¹„ìœ¨ ì ìˆ˜
        silence_score = max(0, 100 - silence_ratio * 200)
        scores.append(silence_score)
        
        # ê¸¸ì´ ì ìˆ˜
        if 3 <= duration <= 30:
            duration_score = 100
        elif duration < 3:
            duration_score = duration / 3 * 100
        else:
            duration_score = max(0, 100 - (duration - 30) * 2)
        scores.append(duration_score)
        
        # í´ë¦¬í•‘ ì ìˆ˜
        if clipping_samples == 0:
            clipping_score = 100
        else:
            clipping_score = max(0, 100 - clipping_samples / 100)
        scores.append(clipping_score)
        
        # í”¼í¬ ë ˆë²¨ ì ìˆ˜
        if 0.3 <= peak_amplitude <= 0.95:
            peak_score = 100
        elif peak_amplitude < 0.3:
            peak_score = peak_amplitude / 0.3 * 100
        else:
            peak_score = max(0, 100 - (peak_amplitude - 0.95) * 200)
        scores.append(peak_score)
        
        # ê°€ì¤‘ í‰ê· 
        weights = [0.2, 0.2, 0.2, 0.2, 0.2]
        final_score = sum(s * w for s, w in zip(scores, weights))
        
        return round(final_score, 1)


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
audio_preprocessor = AudioPreprocessor()
